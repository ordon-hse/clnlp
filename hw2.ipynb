{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "[nltk_data] Downloading package punkt to /home/don/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(fname):\n",
    "    text = ''\n",
    "    with open(fname, 'r') as fin:\n",
    "        text = fin.read()\n",
    "    return text\n",
    "\n",
    "def tokenize_text(text):\n",
    "    sentences = nltk.sent_tokenize(text, language='russian')\n",
    "    for sent in sentences:\n",
    "        tokens.extend(nltk.word_tokenize(sent))\n",
    "    return tokens\n",
    "\n",
    "def tokenize_text_from_file(fname):\n",
    "    tokens = []\n",
    "    text = read_file(fname)\n",
    "    return tokenize_text(text)\n",
    "\n",
    "def get_corpus():\n",
    "    tokens = []\n",
    "    flist = [\n",
    "        'anna-karenina.txt', \n",
    "        'alisa.txt', \n",
    "        'prestuplenie_i_nakazanie.txt', \n",
    "        'mertvye-dushi.txt', \n",
    "        'idiot.txt']\n",
    "    for f in flist:\n",
    "        tokens.extend(tokenize_text(f))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "1029965 1029964\n[('Annotation', '«'), ('«', 'Анна'), ('Анна', 'Каренина'), ('Каренина', '»'), ('»', ','), (',', 'один'), ('один', 'из'), ('из', 'самых'), ('самых', 'знаменитых'), ('знаменитых', 'романов')]\n"
    }
   ],
   "source": [
    "corpus = get_corpus()\n",
    "bigrams = list(nltk.bigrams(corpus))\n",
    "fdist = nltk.FreqDist(corpus)\n",
    "print(len(corpus), len(bigrams))\n",
    "print(bigrams[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _model_add_k_smoothing(model, vocab_len, k=0.00017):\n",
    "    for w1 in model:\n",
    "        frac = float(sum(model[w1].values())) + k*vocab_len\n",
    "        for w2 in model[w1]:\n",
    "            model[w1][w2] = (model[w1][w2] + k) / frac\n",
    "    return model\n",
    "\n",
    "def _model_add_one_smoothing(model, vocab_len):\n",
    "    for w1 in model:\n",
    "        history_count = \n",
    "        frac = float(sum(model[w1].values())) + vocab_len\n",
    "        for w2 in model[w1]:\n",
    "            model[w1][w2] = (model[w1][w2] + 1) / frac\n",
    "    return model\n",
    "\n",
    "def _model_without_smoothing(model):\n",
    "    for w1 in model:\n",
    "        history_count = float(sum(model[w1].values()))\n",
    "        model[w1]['HISTORY_COUNT'] = history_count\n",
    "        for w2 in model[w1]:\n",
    "            model[w1][w2] /= history_count\n",
    "    return model\n",
    "\n",
    "def _mark_unk(corpus, thresh=3):\n",
    "    fdist = nltk.FreqDist(corpus)\n",
    "    unks = [w for w in corpus if fdist[w] < thresh]\n",
    "    for i, w in enumerate(corpus):\n",
    "        if w in unks:\n",
    "            corpus[i] = '<UNK>'\n",
    "    return corpus\n",
    "\n",
    "def train(corpus, n=2, smoothing=None):\n",
    "    \"\"\"\n",
    "    :corpus: word tokenized text (list of words)\n",
    "    :n: n value for n-gram\n",
    "    :smoothing: type of smoothing to apply (None, 'add-one', 'add-k',)\n",
    "    \"\"\"\n",
    "    corpus = _mark_unk(corpus)\n",
    "    vocab = set(corpus)\n",
    "    model = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "    if n == 2:\n",
    "        bigrams = nltk.bigrams(corpus)\n",
    "        for w1, w2 in bigrams:\n",
    "            model[w1][w2] += 1\n",
    "    elif n == 3:\n",
    "        trigrams = nltk.trigrams(corpus)\n",
    "        for w1, w2, w3 in trigrams:\n",
    "            model[(w1, w2)][w3] += 1\n",
    "    if smoothing is None:\n",
    "        return _model_without_smoothing(model)\n",
    "    elif smoothing == 'add-one':\n",
    "        return _model_add_one_smoothing(model, len(vocab))\n",
    "    elif smoothing == 'add-k':\n",
    "        return _model_add_k_smoothing(model, len(vocab))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phrase_probability(model, fdist, vocab, phrase, n=2):\n",
    "    tokens = tokenize_text(phrase)\n",
    "    tokens = _mark_unk(tokens)\n",
    "    prob = 1\n",
    "    if n == 2:\n",
    "        bigrams = nltk.bigrams(tokens)\n",
    "        for w1, w2 in bigrams:\n",
    "            prob *= model[w1][w2] or 1 / (fdist(w1) + len(vocab))\n",
    "    if n == 3:\n",
    "        trigrams = nltk.trigrams(tokens)\n",
    "        for w1, w2, w3 in trigrams:\n",
    "            prob *= model[(w1, w2)][w3] or 1 / (fdist(w1))\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = defaultdict(lambda: defaultdict(lambda: 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w1, w2 in bigrams:\n",
    "    model[w1][w2] += 1\n",
    "\n",
    "for w1 in model:\n",
    "    total_count = float(sum(model[w1].values()))\n",
    "    for w2 in model[w1]:\n",
    "        model[w1][w2] /= total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phrase_prob(phrase):\n",
    "    prob = fdist.freq(phrase[0]) or 1 / len(bigrams)\n",
    "    phrase_bigrams = list(nltk.bigrams(phrase))\n",
    "    for w1, w2 in phrase_bigrams:\n",
    "        prob *= model[w1][w2] or 1 / (fdist.freq(w1) + len(bigrams))\n",
    "        print(f\"model['{w1}']['{w2}']: {model[w1][w2]}\")\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sentence(text_in, word_cnt=10):\n",
    "    text = [t for t in text_in]\n",
    "    sent_finished = False\n",
    "    cnt = 0\n",
    "    while not sent_finished:\n",
    "        r = random.random()\n",
    "        accum = .0\n",
    "        history = text[-1]\n",
    "        for w in model[history].keys():\n",
    "            accum += model[history][w]\n",
    "            if accum >= r:\n",
    "                text.append(w)\n",
    "                cnt += 1\n",
    "                break\n",
    "        if history == (None, None) or word_cnt == cnt:\n",
    "            sent_finished = True\n",
    "\n",
    "    print(' '.join([t for t in text if t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "29050 4643\n[('достоинстве', 2),\n ('Широкого', 2),\n ('XXXV', 2),\n ('пятаяI', 2),\n ('шестаяI', 2),\n ('психологическую', 2),\n ('небывалый', 2),\n ('изображения', 2),\n ('Манн', 2),\n ('эстетической', 2)]\n[(',', 117169),\n ('.', 42756),\n ('и', 35021),\n ('–', 28482),\n ('не', 17585),\n ('в', 16976),\n ('что', 14844),\n ('на', 10154),\n ('он', 10040),\n ('!', 9558)]\n"
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "freqs = fdist.most_common(29050)\n",
    "least = []\n",
    "for f in freqs:\n",
    "    if f[1] < 3:\n",
    "        least.append(f)\n",
    "print(len(freqs), len(least))\n",
    "pprint(least[:10])\n",
    "pprint(fdist.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python36964bitvenvvenve32944a100594504accf8c49d192acd6",
   "display_name": "Python 3.6.9 64-bit ('venv': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}